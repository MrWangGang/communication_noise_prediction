import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import IterableDataset, DataLoader
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import pearsonr
from tqdm import tqdm
import random
import pickle
import matplotlib.pyplot as plt
import os

def set_seed(seed):
    """
    è®¾ç½®æ‰€æœ‰å¯èƒ½çš„éšæœºç§å­ä»¥ç¡®ä¿å®éªŒçš„å¯å¤ç°æ€§
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed) # for multi-GPU
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    print(f"âœ… Random seed set to {seed}")

# ========================= 1. æµå¼æ•°æ®é›†å®šä¹‰ =========================
class TimeSeriesIterableDataset(IterableDataset):
    def __init__(self, file_path, look_back, mean, std, start_row=0, nrows=None, batch_rows=10000):
        """
        :param file_path: CSV æ–‡ä»¶è·¯å¾„
        :param look_back: æ»‘åŠ¨çª—å£é•¿åº¦
        :param mean, std: æ ‡å‡†åŒ–å‚æ•°
        :param start_row: ä»ç¬¬å‡ è¡Œå¼€å§‹è¯»ï¼ˆ0-basedï¼Œä¸å« headerï¼‰
        :param nrows: æ€»å…±è¯»å¤šå°‘è¡Œï¼ˆNone è¡¨ç¤ºåˆ°æ–‡ä»¶æœ«å°¾ï¼‰
        :param batch_rows: æ¯æ¬¡ä» CSV è¯»å¤šå°‘è¡Œ
        """
        self.file_path = file_path
        self.look_back = look_back
        self.mean = mean
        self.std = std
        self.start_row = start_row
        self.nrows = nrows
        self.batch_rows = batch_rows

    def __iter__(self):
        leftover = None
        total_read = 0

        skiprows = 1 + self.start_row

        for chunk in pd.read_csv(
                self.file_path,
                header=None,
                skiprows=skiprows,
                chunksize=self.batch_rows,
                nrows=self.nrows
        ):
            chunk = chunk.values.astype(np.float32)

            if leftover is not None:
                chunk = np.vstack([leftover, chunk])

            leftover = chunk[-self.look_back:].copy()

            chunk = (chunk - self.mean) / self.std

            for i in range(len(chunk) - self.look_back):
                X = chunk[i:i+self.look_back]
                y = chunk[i+self.look_back]
                yield torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)

            total_read += len(chunk) - (0 if leftover is None else self.look_back)
            if self.nrows is not None and total_read >= self.nrows:
                break

class LstmTransformerModel(nn.Module):
    def __init__(self, input_size, lstm_hidden_size=64, lstm_layers=1, dropout=0.1,
                 transformer_nhead=8, transformer_dim_feedforward=256, transformer_layers=1, output_size=None):
        super().__init__()

        self.transformer_embed_dim = transformer_dim_feedforward // 2
        self.lstm_output_dim = lstm_hidden_size * 2

        # LSTM åˆ†æ”¯
        self.lstm = nn.LSTM(
            input_size, lstm_hidden_size, lstm_layers,
            batch_first=True, bidirectional=True, dropout=dropout
        )
        self.lstm_norm = nn.LayerNorm(self.lstm_output_dim)
        self.lstm_output_projection = nn.Linear(self.lstm_output_dim, self.transformer_embed_dim)

        # Transformer åˆ†æ”¯
        self.transformer_input_projection = nn.Linear(input_size, self.transformer_embed_dim)
        self.transformer_encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.transformer_embed_dim, nhead=transformer_nhead,
            dim_feedforward=transformer_dim_feedforward, dropout=dropout, batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=transformer_layers)
        self.transformer_norm = nn.LayerNorm(self.transformer_embed_dim)

        # é—¨æ§èåˆ
        self.gate = nn.Sequential(
            nn.Linear(self.lstm_output_dim + self.transformer_embed_dim, self.transformer_embed_dim),
            nn.Sigmoid()
        )

        self.final_norm = nn.LayerNorm(self.transformer_embed_dim)
        self.dropout = nn.Dropout(dropout)

        # è¾“å‡ºå±‚
        self.fc_out = nn.Linear(self.transformer_embed_dim, output_size if output_size else input_size)

    def forward(self, x):
        # LSTM åˆ†æ”¯
        lstm_out, _ = self.lstm(x)
        lstm_out = self.lstm_norm(lstm_out[:, -1, :])  # æœ€åä¸€ä¸ªæ—¶é—´æ­¥ + LayerNorm
        lstm_out_projected = self.lstm_output_projection(lstm_out)

        # Transformer åˆ†æ”¯
        transformer_input = self.transformer_input_projection(x)
        transformer_out = self.transformer_encoder(transformer_input)
        transformer_out = self.transformer_norm(transformer_out[:, -1, :])  # æœ€åä¸€ä¸ªæ—¶é—´æ­¥ + LayerNorm

        # é—¨æ§èåˆ
        combined = torch.cat((lstm_out, transformer_out), dim=1)
        combined = torch.nan_to_num(combined, nan=0.0, posinf=1e4, neginf=-1e4)
        gate_vector = self.gate(combined)
        fused_out = gate_vector * lstm_out_projected + (1 - gate_vector) * transformer_out
        fused_out = self.final_norm(fused_out)
        fused_out = self.dropout(fused_out)

        return self.fc_out(fused_out)



def calculate_metrics(y_true, y_pred):
    """
    è®¡ç®—å¹¶è¿”å›å„é¡¹è¯„ä»·æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å¯¹ NaN å€¼çš„å¤„ç†ã€‚
    """
    y_true_np = y_true.flatten()
    y_pred_np = y_pred.flatten()

    # æ‰¾åˆ°æ‰€æœ‰é NaN çš„ç´¢å¼•
    valid_indices = np.logical_not(np.isnan(y_true_np) | np.isnan(y_pred_np))

    # ä½¿ç”¨è¿™äº›ç´¢å¼•æ¥è¿‡æ»¤æ•°æ®
    y_true_clean = y_true_np[valid_indices]
    y_pred_clean = y_pred_np[valid_indices]

    if len(y_true_clean) == 0:
        print("Warning: All data points contain NaN. Returning NaN for all metrics.")
        return np.nan, np.nan, np.nan, np.nan, np.nan, np.nan

    mae = mean_absolute_error(y_true_clean, y_pred_clean)
    mse = mean_squared_error(y_true_clean, y_pred_clean)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_true_clean, y_pred_clean)
    r, _ = pearsonr(y_true_clean, y_pred_clean)

    mean_y_true = np.mean(y_true_clean)
    mean_y_pred = np.mean(y_pred_clean)
    std_y_true = np.std(y_true_clean)
    std_y_pred = np.std(y_pred_clean)

    # é¿å…é™¤ä»¥é›¶ï¼Œå¤„ç† std_y_true æˆ– std_y_pred ä¸ºé›¶çš„æƒ…å†µ
    denominator = std_y_true**2 + std_y_pred**2 + (mean_y_true - mean_y_pred)**2
    if denominator == 0:
        ccc = 0.0 # æˆ–è€…å…¶ä»–åˆé€‚çš„å€¼
    else:
        ccc = (2 * r * std_y_true * std_y_pred) / denominator

    return mae, rmse, mse, ccc, r2, r


# ========================= 4. ç»˜å›¾å‡½æ•° =========================
def plot_metrics(train_loss, val_loss, r2, r, rmse, mse, mae, ccc, save_path='training_metrics_lstm_gru.png'):
    plt.figure(figsize=(18, 10))
    plt.subplot(2, 4, 1)
    plt.plot(train_loss, label='Train Loss')
    plt.plot(val_loss,label='Validation Loss')
    plt.title("Loss")
    plt.legend()
    metrics = {
        "R2": r2, "Pearson R": r, "RMSE": rmse, "MSE": mse, "MAE": mae, "CCC": ccc
    }
    for i, (name, values) in enumerate(metrics.items(), 2):
        plt.subplot(2, 4, i)
        plt.plot(values)
        plt.title(name)
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()


# ========================= 5. ä¸»ç¨‹åº =========================
if __name__ == '__main__':
    set_seed(42)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    data_path = './datasets/complex_interference_dataset.csv'
    model_save_path = './best_model_lstm_gru.pth'
    norm_params_path = './norm_params_lstm_gru.pkl'

    max_training_samples = None

    look_back = 10
    batch_size = 64

    # LSTM æ¨¡å‹è¶…å‚æ•°
    lstm_hidden_size = 128
    lstm_layers = 3
    dropout = 0.3

    # Transformer æ¨¡å‹è¶…å‚æ•°
    transformer_nhead = 4
    transformer_dim_feedforward = 512
    transformer_layers = 1

    num_epochs = 100
    learning_rate = 0.001
    patience = 100
    epochs_no_improve = 0

    print(f"Loading data to determine total rows...")
    if max_training_samples is None:
        try:
            total_rows = pd.read_csv(data_path, header=None, skiprows=1).shape[0]
            print(f"Total rows found: {total_rows}")
            data_to_read = total_rows
        except FileNotFoundError:
            print(f"Error: The file {data_path} was not found.")
            exit()
    else:
        data_to_read = max_training_samples

    train_size_rows = int(data_to_read * 0.8)
    val_size_rows = data_to_read - train_size_rows

    if os.path.exists(norm_params_path):
        print(f"âœ… Normalization parameters file found. Loading from {norm_params_path}...")
        with open(norm_params_path, 'rb') as f:
            norm_params = pickle.load(f)
        mean_per_feature = norm_params['mean']
        std_per_feature = norm_params['std']
        first_row = pd.read_csv(data_path, header=None, skiprows=1, nrows=1).values.astype(np.float32)
        input_size = first_row.shape[1]
    else:
        print(f"ğŸš¨ Normalization parameters file not found. Reading training data to compute stats...")
        train_data = pd.read_csv(data_path, header=None, skiprows=1, nrows=train_size_rows).values.astype(np.float32)
        mean_per_feature = np.mean(train_data, axis=0, keepdims=True)
        std_per_feature = np.std(train_data, axis=0, keepdims=True)
        std_per_feature[std_per_feature == 0] = 1e-8
        input_size = train_data.shape[1]
        print(f"Saving normalization parameters to {norm_params_path}...")
        with open(norm_params_path, 'wb') as f:
            pickle.dump({'mean': mean_per_feature, 'std': std_per_feature}, f)
        print("âœ… Normalization parameters saved.")

    output_size = input_size

    train_dataset = TimeSeriesIterableDataset(
        file_path=data_path,
        look_back=look_back,
        mean=mean_per_feature,
        std=std_per_feature,
        start_row=0,
        nrows=train_size_rows,
        batch_rows=5000
    )
    val_dataset = TimeSeriesIterableDataset(
        file_path=data_path,
        look_back=look_back,
        mean=mean_per_feature,
        std=std_per_feature,
        start_row=train_size_rows,
        nrows=val_size_rows,
        batch_rows=5000
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size)
    val_loader = DataLoader(val_dataset, batch_size=batch_size)

    model = LstmTransformerModel(
        input_size=input_size,
        lstm_hidden_size=lstm_hidden_size,
        lstm_layers=lstm_layers,
        dropout=dropout,
        transformer_nhead=transformer_nhead,
        transformer_dim_feedforward=transformer_dim_feedforward,
        transformer_layers=transformer_layers,
        output_size=output_size
    ).to(device)

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5)
    best_val_loss = np.inf
    best_epoch_metrics = {}

    train_loss_list, val_loss_list, mae_list, rmse_list, mse_list, ccc_list, r2_list, r_list = [], [], [], [], [], [], [], []

    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_losses = []
        train_pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Training]", unit="batch")
        for inputs, labels in train_pbar:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)
            optimizer.step()
            train_losses.append(loss.item())
            train_pbar.set_postfix({'loss': loss.item()})
        avg_train_loss = np.mean(train_losses)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_losses = []
        all_preds, all_labels = [], []
        val_pbar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Validation]", unit="batch")
        with torch.no_grad():
            for inputs_val, labels_val in val_pbar:
                inputs_val, labels_val = inputs_val.to(device), labels_val.to(device)
                outputs_val = model(inputs_val)
                loss_val = criterion(outputs_val, labels_val)
                val_losses.append(loss_val.item())
                all_preds.append(outputs_val.cpu().numpy())
                all_labels.append(labels_val.cpu().numpy())
                val_pbar.set_postfix({'loss': loss_val.item()})

        avg_val_loss = np.mean(val_losses)
        y_pred_normalized = np.concatenate(all_preds, axis=0)
        y_true_normalized = np.concatenate(all_labels, axis=0)
        y_pred = y_pred_normalized * std_per_feature + mean_per_feature
        y_true = y_true_normalized * std_per_feature + mean_per_feature
        mae, rmse, mse, ccc, r2, r = calculate_metrics(y_true, y_pred)

        train_loss_list.append(avg_train_loss)
        val_loss_list.append(avg_val_loss)
        mae_list.append(mae)
        rmse_list.append(rmse)
        mse_list.append(mse)
        ccc_list.append(ccc)
        r2_list.append(r2)
        r_list.append(r)

        # æ‰“å°å½“å‰ epoch çš„æŒ‡æ ‡
        print(f"\n--- Epoch {epoch+1}/{num_epochs} ---")
        print(f"Current metrics: Val Loss: {avg_val_loss:.6f}, MAE: {mae:.4f}, RMSE: {rmse:.4f}, CCC: {ccc:.4f}, R2: {r2:.4f} , R: {r:.4f}")

        # æ‰“å°å†å²æœ€ä½³æŒ‡æ ‡
        if best_epoch_metrics:
            print(f"ğŸ† Best metrics so far (Epoch {best_epoch_metrics['epoch']}): Val Loss: {best_epoch_metrics['val_loss']:.6f}, MAE: {best_epoch_metrics['mae']:.4f}, RMSE: {best_epoch_metrics['rmse']:.4f}, CCC: {best_epoch_metrics['ccc']:.4f}, R2: {best_epoch_metrics['r2']:.4f}")

        scheduler.step(avg_val_loss)
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            epochs_no_improve = 0
            torch.save(model.state_dict(), model_save_path)
            best_epoch_metrics = {
                'epoch': epoch + 1,
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'mae': mae,
                'rmse': rmse,
                'mse': mse,
                'ccc': ccc,
                'r2': r2,
                'r': r
            }
            print(f"ğŸ‰ **å·²ä¿å­˜æœ€ä½³æ¨¡å‹** -> Val Loss = {best_val_loss:.6f}")
        else:
            epochs_no_improve += 1
            print(f"No improvement for {epochs_no_improve}/{patience} epochs.")
            if epochs_no_improve >= patience:
                print("ğŸš¨ Early stopping triggered.")
                break

    print("\n" + "="*50)
    print("âœ¨ Training Finished! Final Best Model Metrics:")
    if best_epoch_metrics:
        print(f"Best Epoch: {best_epoch_metrics['epoch']}")
        print(f"Train Loss: {best_epoch_metrics['train_loss']:.6f}, Val Loss: {best_epoch_metrics['val_loss']:.6f}")
        print(f"MAE: {best_epoch_metrics['mae']:.4f}, RMSE: {best_epoch_metrics['rmse']:.4f}, MSE: {best_epoch_metrics['mse']:.4f}")
        print(f"CCC: {best_epoch_metrics['ccc']:.4f}, R2: {best_epoch_metrics['r2']:.4f}, R: {best_epoch_metrics['r']:.4f}")
    else:
        print("No best model was found during training.")
    print("="*50)

    plot_metrics(train_loss_list, val_loss_list, r2_list, r_list, rmse_list, mse_list, mae_list, ccc_list, save_path='training_metrics_lstm_gru.png')